# INTRODUCTION

The pipeline helps in the transfer of data in an organized matter. From the source location to the target location the data are allowed to move by a system known as a data pipeline. Data movements from verified and assigned source to equal assigned target applications is done by data pipeline. An organization can have thousands and thousands of data pipelines. After preprocessing data are gathered together online. 
Data can be in various forms, they are voltage, vibration, sound signals, current, etc. By matching the data various preprocessing are done, they are wavelength transforming, averages, and filtering the data. A very efficient and simple way to keep data and model code in an organized pattern is done by pipelines. In increasing the data's quality "data preprocessing" is the best way by fill the incomplete data, solving the inconsistencies, and by noise smoothness which is occurred due to various reasons.
Sometimes, due to some misunderstanding or due to defective instruments or malfunctions. As the production need becomes high day by day it needs a highly flexible module to organize data. Entries are reduced which results in fewer rows which consideringly results in fewer messages being transported. Thus, for the communication network data load is reduced easily. Between the output data and input data, the inherent dependency which is strong makes it completely impossible to contrast the outputs of one model with another. 
Out of all models that could benefit, automata is chosen as an application as the models are quite sensitive to the size of the input.  Any type of processing done on raw data to get it ready for a further data processing operation is referred to as data preprocessing, which is a part of data preparation. It has historically been a crucial first stage in data mining. Data preparation methods have recently been modified to train AI and machine learning algorithms and to run conclusions against them.
