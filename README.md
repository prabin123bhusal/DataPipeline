# DATA ACQUISITION AND PREPROCESSING  PIPELINE

## INTRODUCTION

The pipeline helps in the transfer of data in an organized matter. From the source location to the target location the data are allowed to move by a system known as a data pipeline. Data movements from verified and assigned source to equal assigned target applications is done by data pipeline. An organization can have thousands and thousands of data pipelines. 

After preprocessing data are gathered together online. Data can be in various forms, they are voltage, vibration, sound signals, current, etc. By matching the data various preprocessing are done, they are wavelength transforming, averages, and filtering the data. A very efficient and simple way to keep data and model code in an organized pattern is done by pipelines. In increasing the data's quality "data preprocessing" is the best way by fill the incomplete data, solving the inconsistencies, and by noise smoothness which is occurred due to various reasons.

Sometimes, due to some misunderstanding or due to defective instruments or malfunctions. As the production need becomes high day by day it needs a highly flexible module to organize data. Entries are reduced which results in fewer rows which consideringly results in fewer messages being transported. Thus, for the communication network data load is reduced easily. Between the output data and input data, the inherent dependency which is strong makes it completely impossible to contrast the outputs of one model with another. 

Out of all models that could benefit, automata is chosen as an application as the models are quite sensitive to the size of the input.  Any type of processing done on raw data to get it ready for a further data processing operation is referred to as data preprocessing, which is a part of data preparation. It has historically been a crucial first stage in data mining. Data preparation methods have recently been modified to train AI and machine learning algorithms and to run conclusions against them.


## IMPLEMENTATION

The paradigm shift is the modified form of the data acquisition processes. It basically depends upon the transition with the combined process of ETL (extract transform load) to the ELT( extract, load, transform). The transition patterns are based on the type of the processing and it is applied in both in cloud based form or offline form.(Shi et.al,2020) The data processing and acquisition are now shifting from the ETL pattern to the ELT pattern. The ETL is reducing the amount of the big intermediate data sets. 

Because the only transformation is triggered by the request for using it further. This data transformation is based on a sub pattern which is called the EtLT, HEre the t stands for the sub transformation. The data is subsequently transformed after the combined extraction and execution. The processes of transformation are executed that are model independent and also transformed into any subsequent model.

ELT has three kinds of processes which are extract, load and transform. Here is the brief discussion of the processes

### Extraction 

The extraction process is the primary step of the data acquisition process. At first all the required data is extracted from the database and then the data has been kept in a staging area. If any transformation is needed in this step it can happen on the staging area so the source data remains constant. The extracted data is moved to the data warehouse.(Roh et.al,2019) The big data warehouse which has a complete ecosystem with the different systems, hardwares, database management systems and communication protocols. 

### Loading 

The loading of the data in the targeted data warehouse is to be completed before the process of the transformation. The loading of the data happens in large numbers from the warehouse. So to handle this large number of the data the loading process needs to be optimized. When the load failure happens the recovery process should be fast so the related operations can restart. The loading should be monitored with respect to the capacity of the server. 

### Transformation 

The data that is retrieved and loaded from the server is raw and useless. It needs optimization, transformation, cleaning, and mapping. This transformation step is the only step which can be customized according to the user. The key feature of this transformation is that the user can use a group of functions to the data which has been extracted and loaded. Direct moved or passed data is the only type of the data which does not require any transformation.

The implementation of this process starts with the “Google Colab console”. In this project the “panda”  library needs to be imported to the software first so we can perform the data acquisition subsequently. The “jupyter notebook” is the best IDE for this process because the predefined  code snippet can help to import and analyze a large number of the datasets.

To create the data pipeline we need to access the web server log data. For the pipeline we need to create a structure that is separate from the other inputs and takes a defined input and in the result it shows a defined output. For the preprocessing pipeline the “Google Colab'' is used to collect and develop the database. The log data that needs to be stored in the database and it also ensures that the data is kept in the database for future analysis. Each component of the pipeline feeds data to the other component. The size of the component needs to be kept small so that anyone can scale the components as much as needed. 


## EVALUATION

The capability to stop any more observations after the first functionality is verified after each occurrence of "cyclic signals" has been checked. A chain of tests is to be done after verifying the Counter module's behavior. Data preprocessing changes the data into a format that can be used in data mining, machine learning, and other data science tasks more quickly and efficiently. To ensure reliable findings, the techniques are typically applied at the very beginning of the machine learning and AI development pipeline. 

Preprocessing data can be done using a variety of tools and techniques, such as the following: sampling, which chooses a sufficient to establish from a huge number of data; process, which modifies raw data to create a single input; noise removal, which eliminates noise from data; accusation, which creates statistically relevant data for information loss; normalization, which arranges data for easier access; and edge detection, which isolates a relevant set of features that is important in a particular context. 

In the first series, the majority of vast signals are transformed into an open chain, then in the second test series, the signals in the majority are modified to figure out cyclic behaviors. A dataset with a low number of "cyclic signals" and predicting its reliability of it can be stopped by the module. Whether the module created has the ability to identify accurately the signals with stable behavior in the case of the "Cyclic Signal Identifier Module". To perform the calculations for the "ACF" process, two "bimodal" signals were created, of which one has a lower variance.

In other words, using a combined method of both is proved to be more useful, as using a multimodal test is very complex and costly compared to a unimodal one.  So unimodal testing is used first. And in any case, if results are not yielded by the unimodal test then the multimodal test is the other alternative option. Profiling of data. Examining, evaluating, and reviewing data in order to compile statistics regarding its accuracy is the process of information profiling. It begins with an analysis of the properties of the current data. 

Data scientists find the data sets that are relevant to the issue at hand, list their key characteristics, and then speculate on which properties might be pertinent for the suggested analytical or machine learning activity. Additionally, they think about which preprocessing packages might be employed and tie data sources to the pertinent business principles.  By various preprocessing steps noise can be filtered out easily. To address this barrier, the phenotype is an elevated clinical and preclinical pipeline for the Python programming language. 

With minimal complexity and background noise, the software enables quick extraction of rising genomic information from digital photos. Phenopype's primary capabilities include data extraction, quick signal processing-based picture preprocessing and segmentation, visualization, and data export. To enable scholarly picture analysis, this capability is offered by encapsulating low-level machine vision libraries (like OpenCV) into usable functions. A project management environment is now available through Phenopype to speed up data collecting and improve reproducibility(Torniainen et al. 2020). 

The data collected has to maintain consistency over a period of time, calculate the "head motion", and establish quantitative models and hence overall quality checking by the procedure of quality control. Another preprocessing step is "Spatial smoothing" which reacts with "STC"(Ahmad et al. 2022).  In the smoothing of the kernel, the time series is altered with the effect of spatial smoothing. Signals having the same unique value are filtered out to enable a good comparison such as: counter signal for two processes. 

To meet the many aspects of big data, various data processing systems have been developed. Before the data is placed in a database system or another storage method, the process of obtaining, filtering, and collecting the data has traditionally been referred to as data acquisition. Four of the Vs—volume, velocity, variety, and value—are most frequently used to guide the collection of big data. The majority of data procurement scenarios scale improvements, rising, elevated, but low-value data, so it's critical to have capable of adapting and time-efficient techniques for information gathering, filtering, and cleaning to guarantee that only the high-value data fractions are actually digested by data gathering.

## DEVELOPMENT STEPS

### DATA ACQUISITION

First process which is needed to carry out the data acquisition process to get the required data chunk from a large database and the source of the data need to be kept intact so the data remains unchanged in the main large database.(Shahbazi et.al 2022) Some little bit of transformation is needed after loading the data from the source. 

An API is created and the exchange rate of the bitcoin’s data set is collected by inputting the specific “URL” in the script after importing the module files. The data set is collected from a verified source and the “requests” script is installed and the dataset is transformed to a “CSV” file by inputting the script in the file.

Then the header files are imported from the “.json” files and the values need to be defined. 
Then the values of the headers and the subsequent data from the “URL” is collected. The response from the .json files have been put on an empty list in the python. Then the headers are defined in the “CSV” file. 

In the next process the element which is defined by the user is added to the empty list. The elements are defined as the name, symbol, and the price of the bitcoins.

Next process we are creating the modified data set and giving it a name and then the encoding of the CSV file is done as UTF8. There is a separation instruction included for printing the data set in a new line. The CSV file is stored in the “Writer” variable and it generates a function which can define the header column data and in the next line the data of the rows will be generated by the “writerrows” function. If all the processes are carried successfully then the program will print done in the terminal. 

### DATA PIPELINE

For the data pipeline system the “panda library” is installed and the “panda” script is imported in the beginning of the coding. Then the data is collected from the “CSV” files and the header number of rows is defined as 10 different types of data. The train test split is imported from the sklearn module of the python for dataset analysis. 

Two variables created one is X and another one is Y. In the Y variable the data frame of a specified elementary is stored and then in the X variable the remaining values excluded from the values stored in the Y variable are stored

From the “train_test” module the 4 kinds of tests are carried out with each variable containing two numbers of tests and trains.(Wittek et.al,2021) In this section the train size is determined as 80% and the test size is determined as 20%. In this position the “random state” is determined as 0.
After this the categorical cols are defined and it is separating the specific amount of the different values which contains characters and numerical calls are also defined for different numbers of numerical values.

Next the importing process of 4 different modules is needed. “Columntranformer”, “Pipeline” “SimpleImputere”, “OneHotEncoder” are the names of the module attributes needed in this code. Next two transformers are created, one is a categorical transformer and another one is categorical transformer. Then these two transformers are defined by the respectable values and put it on the preprocessor variable. Then with the help of the RandomForestregressor module the value of the estimator is defined as 100 and the random state has the value 0. 

Mean absolute error is imported from the “sklearn.metrics” module. Then a variable is created and in that variable both of the X and Y trains are executed. The prediction variable is created and the result of two predefined variables is again tested with the X variable test. Another variable is created as score and in that variable the predictor variable and the test value of Y is also executed and at last the score and mean absolute error is shown in the terminal.  


## RESULT AND ANALYSIS

### DATA ACQUISITION

In this project, there are two distinct tasks to perform. The first one is the “Acquisition of data from API” and the second one is the “Date preprocessing Pipeline”. In the first part, the data has been retrieved from the api with the help of the url “https://api.coincap.io/v2/assets”. The information stored in the destination resembles the data of cryptocurrency. All the retrieved data has been stored into a dataset having a “.csv” format. The dataset upon visualization is visualized in the following format.

In the figure above, the raw data has been displayed. The process of “Data Acquisition” is performed via an API and the information has been stored into a dedicated database using specific programs.

### DATA PREPROCESSING PIPELINE

In this part of the project, the main task is to execute the “Preprocessing Pipeline” operation. The models used in a preprocessing operation using the “Python Programming Language” are aligned properly using the “Pipeline” method. 

In the figure above, the “Raw Dataset” has been displayed in the console after basic preprocessing. The symbols have been converted into numeric values manually in order to avoid unnecessary errors.

In the figure above, the output has been displayed along with the codes. The “Pipeline” function has been used in this part of the project in order to fit the model. The “Preprocessor” variable holds all the data that is gained by transforming the “Categorical Columns” and the “Numerical Columns”.

Finally, the “Machine learning model” is generated by training the 80% data of the dataset and keeping the 20% of the data for the testing purpose. The process has been executed by using the “train_test_split” function. At the end of the process, the model has been fit by using the “Pipeline” function, and the prediction value is compared against the “y_test” value to calculate the “Mean Absolute Error”. The score is printed to the console.


## CONCLUSION

The above paper defines the implementation success of domain or principle knowledge in software so as to deduce the system's behavior's manual effort. The method and the implementation are also provided along with the modules in this paper. With the filter method, the records of "PLC" signals are combined with the "Data Logger".  For changing the value single numbered values are not recorded further in an equal interval of time. 

The changes in the values activate the recordings that send messages to the "Counter Module" that predicts the cyclic pattern of the data. The signal which represents the production cycle completely is identified by the "Cycle Signal Identifier". Now, the next step the preprocessed data process can be executed: the dimension is reduced and then the data streams are converted into datasets by the "Cycle Slicers" that represent the production cycles. The given or selected behavior is measured and tested one by one to check the functionality of each and every module. 

The amount of data to be transferred and the volume of whole data are decreased significantly which as a result decreases the cost of storage and traffic. The next benefit is to make the model less complex by the model convergence faster. The high benefit of the segmented part is required by the models. The "Data Logger Module" is the only procedure excepted for this procedure as it is highly dependent on third-party "SDK" measures the module task to compare different vendors individually.


## REFERENCE LIST

Shi, F., Wang, J., Shi, J., Wu, Z., Wang, Q., Tang, Z., He, K., Shi, Y. and Shen, D., 2020. Review of artificial intelligence techniques in imaging data acquisition, segmentation, and diagnosis for COVID-19. IEEE reviews in biomedical engineering, 14, pp.4-15.

Roh, Y., Heo, G. and Whang, S.E., 2019. A survey on data collection for machine learning: a big data-ai integration perspective. IEEE Transactions on Knowledge and Data Engineering, 33(4), pp.1328-1347.

Shahbazi, Z. and Byun, Y.C., 2022. Knowledge Discovery on Cryptocurrency Exchange Rate Prediction Using Machine Learning Pipelines. Sensors, 22(5), p.1740.

Wittek, K., Wittek, N., Lawton, J., Dohndorf, I., Weinert, A. and Ionita, A., 2021, May. A Blockchain-Based Approach to Provenance and Reproducibility in Research Workflows. In 2021 IEEE International Conference on Blockchain and Cryptocurrency (ICBC) (pp. 1-6). IEEE.

Medium.com (2022), NLP Pipeline, Available at: https://www.google.com/imgres?imgurl=https%3A%2F%2Fmiro.medium.com%2Fmax%2F77%2F1*flOwkNNsfWsoc2QvsNXqnw.png&imgrefurl=https%3A%2F%2Fsunilkumar-prajapati9689.medium.com%2Fnlp-pipeline-27ea54b0f582&tbnid=kvoqI2sjhodUfM&vet=12ahUKEwjRmIOF7YX8AhVXi9gFHYfDCfgQMygJegUIARDPAQ..i&docid=BJdsPbH5z7EX4M&w=773&h=267&q=data%20acquisition%20and%20preprocessing%20pipeline&ved=2ahUKEwjRmIOF7YX8AhVXi9gFHYfDCfgQMygJegUIARDPAQ [Accessed on: 19.12.2022]
https://www.researchgate.net/profile/Angela-Serra-3/publication/340511834/figure/fig1/AS:878106244042753@1586368251370/Example-of-ML-pipeline-for-TGx-data-Data-Acquisition-and-Preprocessing-Data-is.png

Ahmad, A., Parker, D., Dheer, S., Samani, Z.R. and Verma, R., 2022. 3D-QCNet-A Pipeline for Automated Artifact Detection in Diffusion MRI images. Computerized Medical Imaging and Graphics, p.102151.

Torniainen, J., Afara, I.O., Prakash, M., Sarin, J.K., Stenroth, L. and Töyräs, J., 2020. Open-source python module for automated preprocessing of near infrared spectroscopic data. Analytica Chimica Acta, 1108, pp.1-9.














