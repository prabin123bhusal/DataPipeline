# DATA ACQUISITION AND PREPROCESSING  PIPELINE

## INTRODUCTION

The pipeline helps in the transfer of data in an organized matter. From the source location to the target location the data are allowed to move by a system known as a data pipeline. Data movements from verified and assigned source to equal assigned target applications is done by data pipeline. An organization can have thousands and thousands of data pipelines. After preprocessing data are gathered together online. 


Data can be in various forms, they are voltage, vibration, sound signals, current, etc. By matching the data various preprocessing are done, they are wavelength transforming, averages, and filtering the data. A very efficient and simple way to keep data and model code in an organized pattern is done by pipelines. In increasing the data's quality "data preprocessing" is the best way by fill the incomplete data, solving the inconsistencies, and by noise smoothness which is occurred due to various reasons.


Sometimes, due to some misunderstanding or due to defective instruments or malfunctions. As the production need becomes high day by day it needs a highly flexible module to organize data. Entries are reduced which results in fewer rows which consideringly results in fewer messages being transported. Thus, for the communication network data load is reduced easily. Between the output data and input data, the inherent dependency which is strong makes it completely impossible to contrast the outputs of one model with another. 


Out of all models that could benefit, automata is chosen as an application as the models are quite sensitive to the size of the input.  Any type of processing done on raw data to get it ready for a further data processing operation is referred to as data preprocessing, which is a part of data preparation. It has historically been a crucial first stage in data mining. Data preparation methods have recently been modified to train AI and machine learning algorithms and to run conclusions against them.


## IMPLEMENTATION

The paradigm shift is the modified form of the data acquisition processes. It basically depends upon the transition with the combined process of ETL (extract transform load) to the ELT( extract, load, transform). The transition patterns are based on the type of the processing and it is applied in both in cloud based form or offline form. The data processing and acquisition are now shifting from the ETL pattern to the ELT pattern. 

The ETL is reducing the amount of the big intermediate data sets. Because the only transformation is triggered by the request for using it further. This data transformation is based on a sub pattern which is called the EtLT, HEre the t stands for the sub transformation. The data is subsequently transformed after the combined extraction and execution. The processes of transformation are executed that are model independent and also transformed into any subsequent model. 

ELT has three kinds of processes which are extract, load and transform. Here is the brief discussion of the processes

### Extraction 

The extraction process is the primary step of the data acquisition process. At first all the required data is extracted from the database and then the data has been kept in a staging area. If any transformation is needed in this step it can happen on the staging area so the source data remains constant. The extracted data is moved to the data warehouse. The big data warehouse which has a complete ecosystem with the different systems, hardwares, database management systems and communication protocols. 


### Loading 

The loading of the data in the targeted data warehouse is to be completed before the process of the transformation. The loading of the data happens in large numbers from the warehouse. So to handle this large number of the data the loading process needs to be optimized. When the load failure happens the recovery process should be fast so the related operations can restart. The loading should be monitored with respect to the capacity of the server. 


### Transformation 

The data that is retrieved and loaded from the server is raw and useless. It needs optimization, transformation, cleaning, and mapping. This transformation step is the only step which can be customized according to the user. The key feature of this transformation is that the user can use a group of functions to the data which has been extracted and loaded. Direct moved or passed data is the only type of the data which does not require any transformation.


The implementation of this process starts with the **“Google Colab console”**. In this project the **“panda”**  library needs to be imported to the software first so we can perform the data acquisition subsequently. The **“jupyter notebook”** is the best IDE for this process because the predefined  code snippet can help to import and analyze a large number of the datasets. 

To create the data pipeline we need to access the web server log data. For the pipeline we need to create a structure that is separate from the other inputs and takes a defined input and in the result it shows a defined output. For the preprocessing pipeline the **"Google Colab"** is used to collect and develop the database. The log data that needs to be stored in the database and it also ensures that the data is kept in the database for future analysis. Each component of the pipeline feeds data to the other component. The size of the component needs to be kept small so that anyone can scale the components as much as needed.

## EVALUATION

The capability to stop any more observations after the first functionality is verified after each occurrence of "cyclic signals" has been checked. A chain of tests is to be done after verifying the Counter module's behavior. Data preprocessing changes the data into a format that can be used in data mining, machine learning, and other data science tasks more quickly and efficiently. To ensure reliable findings, the techniques are typically applied at the very beginning of the machine learning and AI development pipeline. 

Preprocessing data can be done using a variety of tools and techniques, such as the following: sampling, which chooses a sufficient to establish from a huge number of data; process, which modifies raw data to create a single input; noise removal, which eliminates noise from data; accusation, which creates statistically relevant data for information loss; normalization, which arranges data for easier access; and edge detection, which isolates a relevant set of features that is important in a particular context. 

In the first series, the majority of vast signals are transformed into an open chain, then in the second test series, the signals in the majority are modified to figure out cyclic behaviors. A dataset with a low number of "cyclic signals" and predicting its reliability of it can be stopped by the module. Whether the module created has the ability to identify accurately the signals with stable behavior in the case of the "Cyclic Signal Identifier Module". To perform the calculations for the "ACF" process, two "bimodal" signals were created, of which one has a lower variance.

In other words, using a combined method of both is proved to be more useful, as using a multimodal test is very complex and costly compared to a unimodal one.  So unimodal testing is used first. And in any case, if results are not yielded by the unimodal test then the multimodal test is the other alternative option. Profiling of data. Examining, evaluating, and reviewing data in order to compile statistics regarding its accuracy is the process of information profiling. It begins with an analysis of the properties of the current data. 

Data scientists find the data sets that are relevant to the issue at hand, list their key characteristics, and then speculate on which properties might be pertinent for the suggested analytical or machine learning activity. Additionally, they think about which preprocessing packages might be employed and tie data sources to the pertinent business principles.  By various preprocessing steps noise can be filtered out easily. To address this barrier, the phenotype is an elevated clinical and preclinical pipeline for the Python programming language.

With minimal complexity and background noise, the software enables quick extraction of rising genomic information from digital photos. Phenopype's primary capabilities include data extraction, quick signal processing-based picture preprocessing and segmentation, visualization, and data export. To enable scholarly picture analysis, this capability is offered by encapsulating low-level machine vision libraries (like OpenCV) into usable functions. 

A project management environment is now available through Phenopype to speed up data collecting and improve reproducibility. The data collected has to maintain consistency over a period of time, calculate the "head motion", and establish quantitative models and hence overall quality checking by the procedure of quality control. Another preprocessing step is "Spatial smoothing" which reacts with "STC". 

In the smoothing of the kernel, the time series is altered with the effect of spatial smoothing. Signals having the same unique value are filtered out to enable a good comparison such as: counter signal for two processes.
















